{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bbd64",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchrl.data.replay_buffers import TensorDictReplayBuffer\n",
    "from torchrl.data import LazyMemmapStorage\n",
    "from tensordict import TensorDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a464b4",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self._build_cnn_layers(c, output_dim)\n",
    "        self.target = self._build_cnn_layers(c, output_dim)\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def _build_cnn_layers(self, input_dim, output_dim):\n",
    "        \"\"\"Construct the convolutional layers\"\"\"\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=input_dim, out_channels=32, kernel_size=8, stride=4\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "\n",
    "        return nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            self.conv2,\n",
    "            nn.ReLU(),\n",
    "            self.conv3,\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioAgent:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net.to(self.device)\n",
    "\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_rate_decay = 0.99999975 # slower decay for longer exploration\n",
    "        self.exploration_rate_min = 0.1          # higher min exploration for better exploration\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5 \n",
    "\n",
    "        self.memory = TensorDictReplayBuffer(\n",
    "            storage=LazyMemmapStorage(max_size=100_000, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "\n",
    "        # explore\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        # exploit\n",
    "        else:\n",
    "            state = (\n",
    "                state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            )\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, dim=1).item()\n",
    "\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_rate_min)\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        \n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        self.memory.add(\n",
    "            TensorDict(\n",
    "                {\n",
    "                    \"state\": torch.tensor(state,dtype=torch.float32),\n",
    "                    \"next_state\": torch.tensor(next_state,dtype=torch.float32),\n",
    "                    \"action\": torch.tensor([action],dtype=torch.long),\n",
    "                    \"reward\": torch.tensor([reward],dtype=torch.float32),\n",
    "                    \"done\": torch.tensor([done],dtype=torch.bool),\n",
    "                },\n",
    "                batch_size=[],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device,non_blocking=True)\n",
    "        state, next_state, action, reward, done = (\n",
    "            batch.get(key)\n",
    "            for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\")\n",
    "        )\n",
    "        return state, next_state, action, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab6d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(MarioAgent):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.99\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00007)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        self.burnin = 1e4\n",
    "        self.learn_every = 3\n",
    "        self.sync_every = 1e4\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, next_state, reward, done):\n",
    "        next_state_Q = self.net(next_state, model=\"target\")\n",
    "        best_action = torch.argmax(next_state_Q, dim=1)\n",
    "        next_Q = next_state_Q[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # add gradient clipping\n",
    "        nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=5.0)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    def save_model(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save_model()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # get TD estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "        # get TD target\n",
    "        td_tgt = self.td_target(next_state, reward, done)\n",
    "        # backpropagate loss\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6890f57",
   "metadata": {},
   "source": [
    "# Preprocess & environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from gym.wrappers import FrameStack\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48568b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\n",
    "    \"SuperMarioBros-1-1-v0\", render_mode=\"rgb\", apply_api_compatibility=True\n",
    ")\n",
    "\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=(84, 84))\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "# done = True\n",
    "# for step in range(5000):\n",
    "#     if done:\n",
    "#         state = env.reset()\n",
    "#         env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "\n",
    "print('action space',env.action_space.n)\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make(\n",
    "    \"SuperMarioBros-1-1-v0\", render_mode=\"rgb_array\", apply_api_compatibility=True\n",
    ")\n",
    "\n",
    "# limit actions and apply wrappers\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=(84, 84))\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "episodes = 400\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = mario.act(state)\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "        q, loss = mario.learn()\n",
    "        state = next_state\n",
    "\n",
    "        if done or info[\"flag_get\"]:\n",
    "            print(\n",
    "                f\"Episode: {e}, \"\n",
    "                f\"Step: {mario.curr_step}, \"\n",
    "                f\"Exploration Rate: {mario.exploration_rate:.5f}, \"\n",
    "                f\"Q: {q}, \"\n",
    "                f\"Loss: {loss}\"\n",
    "            )\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236fafa",
   "metadata": {},
   "source": [
    "### Making it play the game !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc7122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
